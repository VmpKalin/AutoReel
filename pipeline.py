#!/usr/bin/env python3
"""
Video Processing Pipeline
=========================
1. Extract audio
2. Enhance audio (Auphonic)
3. Transcription (WhisperX) â†’ save locally
4. Text correction (Claude API)
5. Subtitle generation (SRT + ASS)
6. Burn subtitles into video
7. Video formatting (padding, crop)
8. Metadata generation for Instagram + TikTok
"""

import os
import json
import time
import shutil
import logging
import argparse
import subprocess
import requests
import ollama
from pathlib import Path
from datetime import timedelta
from dotenv import load_dotenv
import anthropic
from moviepy import VideoFileClip, TextClip, CompositeVideoClip
import pysrt

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("pipeline.log"),
    ],
)
log = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Config:
    # Auphonic
    AUPHONIC_API_KEY = os.getenv("AUPHONIC_API_KEY", "")
    AUPHONIC_PRESET  = os.getenv("AUPHONIC_PRESET", "")

    # Anthropic
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

    # WhisperX
    WHISPER_MODEL    = os.getenv("WHISPER_MODEL", "large-v3")
    WHISPER_DEVICE   = os.getenv("WHISPER_DEVICE", "cpu")
    WHISPER_LANGUAGE = os.getenv("WHISPER_LANGUAGE", "uk")

    # Subtitles
    SUBTITLE_FONT          = os.getenv("SUBTITLE_FONT", "Arial")
    SUBTITLE_FONT_SIZE     = int(os.getenv("SUBTITLE_FONT_SIZE", "18"))
    SUBTITLE_COLOR         = os.getenv("SUBTITLE_COLOR", "&H00FFFFFF")
    SUBTITLE_OUTLINE_COLOR = os.getenv("SUBTITLE_OUTLINE_COLOR", "&H00000000")
    SUBTITLE_OUTLINE_SIZE  = int(os.getenv("SUBTITLE_OUTLINE_SIZE", "2"))
    SUBTITLE_POSITION      = os.getenv("SUBTITLE_POSITION", "bottom")

    # Video format
    OUTPUT_FORMAT = os.getenv("OUTPUT_FORMAT", "16:9")
    ADD_PADDING   = os.getenv("ADD_PADDING", "false").lower() == "true"
    PADDING_COLOR = os.getenv("PADDING_COLOR", "black")
    
    CONVERT_TO_1080P = os.getenv("CONVERT_TO_1080P", "true").lower() == "true"

    REMOVE_AUPHONIC_WATERMARK = os.getenv("REMOVE_AUPHONIC_WATERMARK", "true").lower() == "true"

cfg = Config()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI Prompts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRANSCRIPT_FIX_PROMPT = """You are a professional transcript editor.
Your task is to fix grammar, punctuation and spelling errors in each line.

Rules:
- KEEP the [N] numbering and structure exactly as is
- DO NOT change the meaning, word order or content
- Fix punctuation, capitalization and obvious speech-to-text errors
- Do NOT add periods at the end of lines â€” subtitles should not end with a dot
- Return ONLY the corrected lines in format [N] text
- Nothing else, no explanations"""


METADATA_PROMPT = """You are a social media content strategist specializing in Instagram and TikTok growth.
Based on the video transcript, generate optimized metadata for maximum reach and engagement.

Return ONLY JSON, no markdown:
{
  "title": "Catchy video title under 60 chars, curiosity-driven",
  "instagram_caption": "Engaging caption with hook, value, CTA and relevant hashtags. Max 2200 chars. Use emojis naturally.",
  "instagram_hashtags": ["hashtag1", "hashtag2"],
  "tiktok_caption": "Short punchy caption under 150 chars with 3-5 hashtags. Hook in first line.",
  "tiktok_hashtags": ["hashtag1", "hashtag2"],
  "short_summary": "2 sentences describing the video content"
}

Hashtag strategy:
- Instagram: mix of niche (10k-100k), medium (100k-1M) and broad tags
- TikTok: trending + niche specific tags
- Always include topic-relevant tags, avoid generic spam tags"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run(cmd: str, desc: str = "") -> str:
    log.info(f"âš™ï¸  {desc or cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        log.error(f"STDERR: {result.stderr}")
        raise RuntimeError(f"Command failed: {cmd}\n{result.stderr}")
    return result.stdout.strip()


def ensure_dir(path: Path) -> Path:
    path.mkdir(parents=True, exist_ok=True)
    return path


def format_srt_time(seconds: float) -> str:
    td = timedelta(seconds=seconds)
    total_ms = int(td.total_seconds() * 1000)
    h  = total_ms // 3_600_000
    m  = (total_ms % 3_600_000) // 60_000
    s  = (total_ms % 60_000) // 1000
    ms = total_ms % 1000
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"


def format_ass_time(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = seconds % 60
    return f"{h}:{m:02d}:{s:05.2f}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 1 â€” Extract Audio
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_audio(video_path: Path, output_dir: Path) -> Path:
    audio_path = output_dir / "audio_original.wav"
    run(
        f'ffmpeg -y -i "{video_path}" -vn -acodec pcm_s16le -ar 44100 -ac 1 "{audio_path}"',
        "Extract audio from video",
    )
    log.info(f"âœ… Audio saved: {audio_path}")
    return audio_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 2 â€” Enhance Audio via Auphonic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enhance_audio_auphonic(audio_path: Path, output_dir: Path) -> Path:
    if not cfg.AUPHONIC_API_KEY:
        log.warning("âš ï¸  AUPHONIC_API_KEY is not set, skipping audio enhancement")
        return audio_path

    log.info("ğŸ”Š Uploading audio to Auphonic...")
    headers = {"Authorization": f"Bearer {cfg.AUPHONIC_API_KEY}"}

    data = {
        "action": "start",
        "output_basename": "enhanced",
        "algorithms": json.dumps({
            "normloudness": True,
            "denoise": True,
            "denoiseamount": 0.8,
            "hiss_reduction": True,
        }),
    }
    if cfg.AUPHONIC_PRESET:
        data["preset"] = cfg.AUPHONIC_PRESET

    with open(audio_path, "rb") as f:
        resp = requests.post(
            "https://auphonic.com/api/simple/productions.json",
            headers=headers,
            data=data,
            files={"input_file": f},
        )
    resp.raise_for_status()
    uuid = resp.json()["data"]["uuid"]
    log.info(f"ğŸ“¤ Production UUID: {uuid}")

    # Poll until completion
    log.info("â³ Waiting for Auphonic to finish...")
    prod_data = {}
    for _ in range(120):
        time.sleep(5)
        status_resp = requests.get(
            f"https://auphonic.com/api/production/{uuid}.json", headers=headers
        )
        status_resp.raise_for_status()
        prod_data = status_resp.json()["data"]
        log.info(f"   Status: {prod_data.get('status_string', '')}")
        if prod_data["status"] == 3:
            break
        if prod_data["status"] in (9, 10):
            raise RuntimeError(f"Auphonic failed: {prod_data.get('status_string')}")

    output_files = prod_data.get("output_files", [])
    if not output_files:
        raise RuntimeError("Auphonic did not return output files")

    enhanced_path = output_dir / "audio_enhanced.wav"
    log.info("ğŸ“¥ Downloading enhanced audio...")
    audio_resp = requests.get(output_files[0]["download_url"], headers=headers)
    audio_resp.raise_for_status()
    with open(enhanced_path, "wb") as f:
        f.write(audio_resp.content)

    log.info(f"âœ… Enhanced audio saved: {enhanced_path}")
    return enhanced_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 3 â€” Merge Enhanced Audio into Video
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_audio_video(video_path: Path, audio_path: Path, output_dir: Path) -> Path:
    output_path = output_dir / "video_enhanced.mp4"
    run(
        f'ffmpeg -y -i "{video_path}" -i "{audio_path}" '
        f'-c:v copy -map 0:v:0 -map 1:a:0 -shortest "{output_path}"',
        "Merge video with enhanced audio",
    )
    log.info(f"âœ… Video with new audio: {output_path}")
    return output_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 4 â€” Transcription (WhisperX) â†’ save locally
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def transcribe(audio_path: Path, output_dir: Path) -> list[dict]:
    try:
        import whisperx
    except ImportError:
        raise ImportError("WhisperX is not installed. Run: pip install whisperx torch")

    device = cfg.WHISPER_DEVICE
    log.info(f"ğŸ™ï¸  WhisperX transcription (model={cfg.WHISPER_MODEL}, device={device})...")

    model  = whisperx.load_model(cfg.WHISPER_MODEL, device=device, compute_type="float32")
    audio  = whisperx.load_audio(str(audio_path))
    result = model.transcribe(audio, language=cfg.WHISPER_LANGUAGE, batch_size=16)

    log.info("ğŸ”¡ Word-level alignment...")
    model_a, metadata = whisperx.load_align_model(
        language_code=cfg.WHISPER_LANGUAGE, device=device
    )
    result = whisperx.align(
        result["segments"], model_a, metadata, audio, device,
        return_char_alignments=False,
    )

    # Build short segments directly from word_segments
    word_segments = result.get("word_segments", [])
    segments = []

    if word_segments:
        log.info(f"âœ‚ï¸  Building subtitles from {len(word_segments)} words...")
        MAX_CHARS    = 42
        MAX_DURATION = 3.5

        chunk_words  = []
        chunk_start  = word_segments[0]["start"]

        for word in word_segments:
            # Skip words without timestamps
            if "start" not in word or "end" not in word:
                chunk_words.append(word["word"])
                continue

            chunk_words.append(word["word"])
            chunk_text = " ".join(chunk_words)
            chunk_dur  = word["end"] - chunk_start

            if len(chunk_text) >= MAX_CHARS or chunk_dur >= MAX_DURATION:
                segments.append({
                    "start": chunk_start,
                    "end":   word["end"],
                    "text":  chunk_text,
                })
                chunk_words = []
                chunk_start = word["end"]

        if chunk_words:
            last_end = word_segments[-1].get("end", chunk_start + 1)
            segments.append({
                "start": chunk_start,
                "end":   last_end,
                "text":  " ".join(chunk_words),
            })
    else:
        # Fallback â€” use regular segments
        segments = result["segments"]

    log.info(f"âœ… Transcription: {len(segments)} subtitle segments")

    # Save files
    raw_json = output_dir / "transcript_raw.json"
    with open(raw_json, "w", encoding="utf-8") as f:
        json.dump(segments, f, ensure_ascii=False, indent=2)

    raw_txt = output_dir / "transcript.txt"
    with open(raw_txt, "w", encoding="utf-8") as f:
        for seg in segments:
            f.write(f"[{format_srt_time(seg['start'])} â†’ {format_srt_time(seg['end'])}] {seg['text'].strip()}\n")

    log.info(f"ğŸ’¾ Transcript: {raw_json}")
    return segments


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 5 â€” Fix Transcript via Claude
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fix_transcript(segments: list[dict], output_dir: Path) -> list[dict]:

    log.info("âœï¸  Fixing text with local model...")
    full_text = "\n".join(f"[{i}] {seg['text']}" for i, seg in enumerate(segments))

    message = ollama.chat(
        model="llama3.1:8b",
        messages=[
            {"role": "system", "content": TRANSCRIPT_FIX_PROMPT},
            {"role": "user", "content": full_text},
        ]
    )

    corrected_lines = {}
    response_text = message['message']['content']
    for line in response_text.strip().split("\n"):
        line = line.strip()
        if line.startswith("["):
            try:
                idx_end = line.index("]")
                idx  = int(line[1:idx_end])
                text = line[idx_end + 1:].strip()
                corrected_lines[idx] = text
            except (ValueError, IndexError):
                continue

    for i, seg in enumerate(segments):
        if i in corrected_lines:
            seg["text"] = corrected_lines[i]

    # Save corrected version
    fixed_json = output_dir / "transcript_fixed.json"
    with open(fixed_json, "w", encoding="utf-8") as f:
        json.dump(segments, f, ensure_ascii=False, indent=2)

    fixed_txt = output_dir / "transcript_fixed.txt"
    with open(fixed_txt, "w", encoding="utf-8") as f:
        for seg in segments:
            f.write(f"[{format_srt_time(seg['start'])} â†’ {format_srt_time(seg['end'])}] {seg['text'].strip()}\n")

    log.info(f"âœ… Corrected transcript: {fixed_json}")
    return segments


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 6 â€” Generate Subtitles (SRT + ASS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_srt(segments: list[dict], output_dir: Path) -> Path:
    srt_path = output_dir / "subtitles.srt"
    lines = []
    for i, seg in enumerate(segments, 1):
        lines.append(
            f"{i}\n{format_srt_time(seg['start'])} --> {format_srt_time(seg['end'])}\n{seg['text'].strip()}\n"
        )
    with open(srt_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    log.info(f"âœ… SRT subtitles: {srt_path}")
    return srt_path


def generate_ass(segments: list[dict], output_dir: Path) -> Path:
    ass_path = output_dir / "subtitles.ass"
    alignment = {"bottom": 2, "top": 8, "center": 5}.get(cfg.SUBTITLE_POSITION, 2)

    header = (
        "[Script Info]\nScriptType: v4.00+\nWrapStyle: 0\nScaledBorderAndShadow: yes\n"
        "PlayResX: 1920\nPlayResY: 1080\n\n"
        "[V4+ Styles]\n"
        "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, "
        "Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, "
        "Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n"
        f"Style: Default,{cfg.SUBTITLE_FONT},{cfg.SUBTITLE_FONT_SIZE},"
        f"{cfg.SUBTITLE_COLOR},&H000000FF,{cfg.SUBTITLE_OUTLINE_COLOR},&H00000000,"
        f"0,0,0,0,100,100,0,0,1,{cfg.SUBTITLE_OUTLINE_SIZE},0,{alignment},10,10,30,1\n\n"
        "[Events]\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n"
    )
    events = [
        f"Dialogue: 0,{format_ass_time(s['start'])},{format_ass_time(s['end'])},"
        f"Default,,0,0,0,,{s['text'].strip().replace(chr(10), chr(92)+'N')}"
        for s in segments
    ]
    with open(ass_path, "w", encoding="utf-8") as f:
        f.write(header + "\n".join(events))
    log.info(f"âœ… ASS subtitles: {ass_path}")
    return ass_path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 7 â€” Burn Subtitles into Video
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def burn_subtitles(video_path: Path, srt_path: Path, output_dir: Path) -> Path:
    output_path = output_dir / "video_subtitled.mp4"
    log.info("ğŸ¬ Burning subtitles via Python (Pillow + MoviePy)...")

    env = os.environ.copy()
    env["PATH"] = "/opt/homebrew/bin:/usr/local/bin:/usr/bin:/bin:" + env.get("PATH", "")

    working_path = output_dir / "video_h264.mp4"
    if cfg.CONVERT_TO_1080P:
        if not working_path.exists():
            log.info("ğŸ”„ Converting HEVC â†’ h264 1080p...")
            subprocess.run(
                f'ffmpeg -y -i "{video_path}" -c:v libx264 -crf 18 -preset fast '
                f'-vf "scale=1080:1920" -c:a aac "{working_path}"',
                shell=True, env=env, capture_output=True
            )
            log.info("âœ… Conversion complete")
        video = VideoFileClip(str(working_path))
    else:
        log.info("â© Conversion skipped (CONVERT_TO_1080P=false)")
        video = VideoFileClip(str(video_path))

    subs = pysrt.open(str(srt_path))

    subtitle_clips = []
    for sub in subs:
        start = sub.start.ordinal / 1000.0
        end   = sub.end.ordinal / 1000.0
        duration = end - start

        # Automatically scale font relative to video height
        auto_font_size = max(cfg.SUBTITLE_FONT_SIZE, int(video.h * 0.045))

        txt_clip = (
            TextClip(
                text=sub.text.strip(),
                font="/System/Library/Fonts/Supplemental/Arial.ttf",
                font_size=auto_font_size,
                color="white",
                stroke_color="black",
                stroke_width=max(cfg.SUBTITLE_OUTLINE_SIZE, 3),
                method="caption",
                size=(int(video.w * 0.85), None),
            )
            .with_start(start)
            .with_duration(duration)
        )

        # Subtitle position
        if cfg.SUBTITLE_POSITION == "top":
            pos = ("center", 50)
        elif cfg.SUBTITLE_POSITION == "center":
            pos = ("center", "center")
        else:  # bottom
            pos = ("center", video.h - txt_clip.h - 60)

        subtitle_clips.append(txt_clip.with_position(pos))

    final = CompositeVideoClip([video, *subtitle_clips])
    final.write_videofile(
        str(output_path),
        codec="libx264",
        audio_codec="aac",
        logger=None,
    )

    log.info(f"âœ… Video with subtitles: {output_path}")
    return output_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 8 â€” Format Video (padding / crop)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ASPECT_RATIOS = {
    "16:9": (1920, 1080),
    "9:16": (1080, 1920),
    "1:1":  (1080, 1080),
    "4:5":  (1080, 1350),
}


def format_video(video_path: Path, output_dir: Path, suffix: str = "formatted") -> Path:
    if cfg.OUTPUT_FORMAT == "original" and not cfg.ADD_PADDING:
        log.info("â© Formatting skipped (original)")
        return video_path

    output_path = output_dir / f"video_{suffix}.mp4"
    w, h = ASPECT_RATIOS.get(cfg.OUTPUT_FORMAT, (1920, 1080))

    if cfg.ADD_PADDING:
        vf = (
            f"scale={w}:{h}:force_original_aspect_ratio=decrease,"
            f"pad={w}:{h}:(ow-iw)/2:(oh-ih)/2:{cfg.PADDING_COLOR}"
        )
    else:
        vf = f"scale={w}:{h}:force_original_aspect_ratio=increase,crop={w}:{h}"

    run(
        f'ffmpeg -y -i "{video_path}" -vf "{vf}" -c:a copy "{output_path}"',
        f"Video formatting â†’ {cfg.OUTPUT_FORMAT}",
    )
    log.info(f"âœ… Formatted video: {output_path}")
    return output_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 9 â€” Generate Metadata (Instagram + TikTok)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_metadata(segments: list[dict], output_dir: Path) -> dict:
    if not cfg.ANTHROPIC_API_KEY:
        log.warning("âš ï¸  ANTHROPIC_API_KEY is not set, skipping metadata generation")
        return {}

    log.info("ğŸ“‹ Generating metadata (Instagram + TikTok) via Claude...")
    client = anthropic.Anthropic(api_key=cfg.ANTHROPIC_API_KEY)

    transcript_preview = " ".join(seg["text"] for seg in segments)[:3000]

    message = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=1500,
        messages=[
            {"role": "user", "content": METADATA_PROMPT},
            {"role": "assistant", "content": "Understood. Please provide the video transcript."},
            {"role": "user", "content": f"Video transcript:\n{transcript_preview}"},
        ],
    )

    raw = message.content[0].text.strip().replace("```json", "").replace("```", "").strip()
    try:
        metadata = json.loads(raw)
    except json.JSONDecodeError:
        log.warning("âš ï¸  Failed to parse metadata as JSON")
        metadata = {"raw": raw}

    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    log.info(f"âœ… Metadata saved: {metadata_path}")
    return metadata


def print_metadata(metadata: dict):
    if not metadata:
        return
    print("\n" + "=" * 55)
    print("ğŸ“‹ GENERATED METADATA")
    print("=" * 55)
    list_fields = {"instagram_hashtags", "tiktok_hashtags"}
    for k, v in metadata.items():
        if k in list_fields:
            print(f"\nğŸ”¹ {k.upper()}:\n   {' '.join('#' + t for t in v)}")
        else:
            print(f"\nğŸ”¹ {k.upper()}:\n{v}")
    print()

AUPHONIC_WATERMARK_KEYWORDS = [
    "auphonic", "biophonic", "phonic", 
    "free audio", "post-production", "post production"
]

def trim_auphonic_watermark(audio_path: Path, output_dir: Path) -> Path:
    """Removes Auphonic watermark from the beginning of audio."""
    try:
        import whisperx
    except ImportError:
        return audio_path

    log.info("ğŸ” Searching for Auphonic watermark in audio...")

    # Quick transcription of only the first 15 seconds
    model = whisperx.load_model("base", device=cfg.WHISPER_DEVICE, compute_type="float32")
    audio = whisperx.load_audio(str(audio_path))
    audio_preview = audio[:15 * 16000]  # first 15 seconds
    result = model.transcribe(audio_preview, language="en", batch_size=16)

    # Find where watermark ends
    cut_time = 0.0
    for seg in result["segments"]:
        text_lower = seg["text"].lower()
        is_watermark = any(kw in text_lower for kw in AUPHONIC_WATERMARK_KEYWORDS)
        if is_watermark:
            cut_time = seg["end"]
            log.info(f"ğŸ¯ Watermark found: '{seg['text'].strip()}' â†’ trimming to {cut_time:.1f}s")

    if cut_time == 0.0:
        log.info("âœ… Watermark not found")
        return audio_path

    # Trim audio
    trimmed_path = output_dir / "audio_trimmed.wav"
    env = os.environ.copy()
    env["PATH"] = "/opt/homebrew/bin:/usr/local/bin:/usr/bin:/bin:" + env.get("PATH", "")
    subprocess.run(
        f'ffmpeg -y -i "{audio_path}" -ss {cut_time:.3f} -c copy "{trimmed_path}"',
        shell=True, env=env, capture_output=True
    )
    log.info(f"âœ… Audio trimmed from {cut_time:.1f}s: {trimmed_path}")
    return trimmed_path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Pipeline
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_pipeline(input_video: str, output_dir: str, steps: list[str] = None):
    video_path = Path(input_video).resolve()
    if not video_path.exists():
        raise FileNotFoundError(f"Video not found: {video_path}")

    out = ensure_dir(Path(output_dir))
    log.info(f"ğŸš€ Starting pipeline for: {video_path.name}")
    log.info(f"ğŸ“ Output directory: {out}")

    all_steps = ["audio", "enhance", "remove_watermark", "merge", "transcribe", "fix", "subtitles", "format", "metadata"]
    active = set(steps) if steps else set(all_steps)
    state  = {"video": video_path}

    # 1. Extract audio
    if "audio" in active:
        state["audio"] = extract_audio(state["video"], out)

    # 2. Enhance audio
    if "enhance" in active and "audio" in state:
        state["enhanced_audio"] = enhance_audio_auphonic(state["audio"], out)
    else:
        state["enhanced_audio"] = state.get("audio", video_path)

    if "remove_watermark" in active and "audio" in state and cfg.REMOVE_AUPHONIC_WATERMARK:
        state["enhanced_audio"] = trim_auphonic_watermark(state["enhanced_audio"], out)

    # 3. Merge video + audio
    if "merge" in active and state.get("enhanced_audio") != state.get("audio"):
        state["video"] = merge_audio_video(state["video"], state["enhanced_audio"], out)
    else:
        # Priority: subtitled â†’ enhanced â†’ original
        video_subtitled = out / "video_subtitled.mp4"
        video_enhanced  = out / "video_enhanced.mp4"

        if video_subtitled.exists() and "subtitles" not in active:
            state["video"] = video_subtitled
            log.info(f"ğŸ“‚ Using existing subtitled video: {video_subtitled}")
        elif video_enhanced.exists():
            state["video"] = video_enhanced
            log.info(f"ğŸ“‚ Using enhanced video: {video_enhanced}")
        else:
            dest = out / video_path.name
            if not dest.exists():
                shutil.copy2(video_path, dest)
            state["video"] = dest
            log.info(f"ğŸ“‚ Using original video: {dest}")
        
    # Load existing transcript if present
    fixed_json = out / "transcript_fixed.json"
    raw_json = out / "transcript_raw.json"
    if "transcribe" not in active:
        if fixed_json.exists():
            with open(fixed_json, encoding="utf-8") as f:
                state["segments"] = json.load(f)
            log.info(f"ğŸ“‚ Loaded existing transcript: {fixed_json}")
        elif raw_json.exists():
            with open(raw_json, encoding="utf-8") as f:
                state["segments"] = json.load(f)
            log.info(f"ğŸ“‚ Loaded existing transcript: {raw_json}")

    # 4. Transcription â†’ save locally
    if "transcribe" in active:
        audio_src = state.get("enhanced_audio") or state.get("audio")
        state["segments"] = transcribe(audio_src, out)

    # 5. Text correction
    if "fix" in active and "segments" in state:
        state["segments"] = fix_transcript(state["segments"], out)

    # 6. Subtitles
    if "subtitles" in active and "segments" in state:
        srt_path = generate_srt(state["segments"], out)
        generate_ass(state["segments"], out)
        state["video"] = burn_subtitles(state["video"], srt_path, out)

    # 7. Video formatting
    if "format" in active:
        state["video"] = format_video(state["video"], out)

    # 8. Instagram + TikTok metadata
    if "metadata" in active and "segments" in state:
        state["metadata"] = generate_metadata(state["segments"], out)
        print_metadata(state["metadata"])

    log.info("\n" + "=" * 55)
    log.info("ğŸ‰ PIPELINE COMPLETED!")
    log.info(f"ğŸ“¹ Final video : {state['video']}")
    log.info(f"ğŸ“ All files   : {out}")
    return state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(
        description="Video pipeline: audio â†’ transcription â†’ subtitles â†’ metadata"
    )
    parser.add_argument("input", help="Path to input video file")
    parser.add_argument("-o", "--output", default="./output",
                        help="Output directory (default: ./output)")
    parser.add_argument(
        "--steps", nargs="+",
        choices=["audio", "enhance", "remove_watermark", "merge", "transcribe", "fix", "subtitles", "format", "metadata"],
        help="Run only specified steps (default: all)",
    )
    args = parser.parse_args()
    run_pipeline(args.input, args.output, args.steps)


if __name__ == "__main__":
    main()