#!/usr/bin/env python3
"""
Video Processing Pipeline
=========================
ĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ²Ñ–Ğ´ĞµĞ¾:
1. Ğ’Ğ¸Ñ‚ÑĞ³ Ğ°ÑƒĞ´Ñ–Ğ¾
2. ĞŸĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ Ğ·Ğ²ÑƒĞºÑƒ (Auphonic)
3. Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ñ–Ñ (WhisperX / Deepgram)
4. Ğ’Ğ¸Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ Ñ‚ĞµĞºÑÑ‚Ñƒ (Claude API)
5. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ñ–Ğ²
6. ĞĞ°ĞºĞ»Ğ°Ğ´Ğ°Ğ½Ğ½Ñ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ñ–Ğ² Ğ½Ğ° Ğ²Ñ–Ğ´ĞµĞ¾
7. Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ²Ñ–Ğ´ĞµĞ¾ (Ñ€Ğ°Ğ¼ĞºĞ°, crop)
8. ĞĞ°Ñ€Ñ–Ğ·ĞºĞ° Reels
9. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ Ğ½Ğ°Ğ·Ğ²Ğ¸, Ğ¿Ñ–Ğ´Ğ¿Ğ¸ÑÑƒ, Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ…
"""

import os
import sys
import json
import time
import shutil
import logging
import argparse
import subprocess
import requests
from pathlib import Path
from datetime import timedelta
from typing import Optional
from dotenv import load_dotenv
import anthropic

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("pipeline.log"),
    ],
)
log = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Config:
    # Auphonic
    AUPHONIC_USER = os.getenv("AUPHONIC_USER", "")
    AUPHONIC_PASS = os.getenv("AUPHONIC_PASS", "")
    AUPHONIC_PRESET = os.getenv("AUPHONIC_PRESET", "")  # optional preset UUID

    # Deepgram (Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° WhisperX ÑĞºÑ‰Ğ¾ Ğ½ĞµĞ¼Ğ°Ñ” GPU)
    DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY", "")

    # Whisper
    WHISPER_MODEL = os.getenv("WHISPER_MODEL", "large-v3")  # tiny/base/small/medium/large-v3
    WHISPER_DEVICE = os.getenv("WHISPER_DEVICE", "cpu")     # cpu / cuda
    WHISPER_LANGUAGE = os.getenv("WHISPER_LANGUAGE", "uk")  # uk / ru / en

    # Subtitles style
    SUBTITLE_FONT = os.getenv("SUBTITLE_FONT", "Arial")
    SUBTITLE_FONT_SIZE = int(os.getenv("SUBTITLE_FONT_SIZE", "18"))
    SUBTITLE_COLOR = os.getenv("SUBTITLE_COLOR", "&H00FFFFFF")  # white
    SUBTITLE_OUTLINE_COLOR = os.getenv("SUBTITLE_OUTLINE_COLOR", "&H00000000")  # black
    SUBTITLE_OUTLINE_SIZE = int(os.getenv("SUBTITLE_OUTLINE_SIZE", "2"))
    SUBTITLE_POSITION = os.getenv("SUBTITLE_POSITION", "bottom")  # bottom / top / center

    # Video format
    OUTPUT_FORMAT = os.getenv("OUTPUT_FORMAT", "16:9")  # 16:9 / 9:16 / 1:1 / original
    ADD_PADDING = os.getenv("ADD_PADDING", "false").lower() == "true"
    PADDING_COLOR = os.getenv("PADDING_COLOR", "black")

    # Reels
    REELS_COUNT = int(os.getenv("REELS_COUNT", "3"))
    REELS_MIN_DURATION = int(os.getenv("REELS_MIN_DURATION", "30"))
    REELS_MAX_DURATION = int(os.getenv("REELS_MAX_DURATION", "60"))

    # Transcription backend: "whisperx" / "deepgram"
    TRANSCRIPTION_BACKEND = os.getenv("TRANSCRIPTION_BACKEND", "whisperx")


cfg = Config()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI Prompts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRANSCRIPT_FIX_PROMPT = """You are a professional transcript editor.
Your task is to fix grammar, punctuation and spelling errors in each line.

Rules:
- KEEP the [N] numbering and structure exactly as is
- DO NOT change the meaning, word order or content
- Fix punctuation, capitalization and obvious speech-to-text errors
- Return ONLY the corrected lines in format [N] text
- Nothing else, no explanations"""


REELS_SELECTION_PROMPT = """You are a viral social media video editor specializing in Instagram Reels and TikTok.
Analyze the transcript and find the most engaging moments for short-form content.

What makes a great Reel/TikTok moment:
- Strong hook in the first 3 seconds
- Emotional or surprising moments
- Actionable tips or insights
- Funny or relatable content
- Clear standalone value (no context needed)
- Energetic or passionate delivery

Return ONLY a JSON array, no markdown:
[{"start": 12.5, "end": 45.0, "title": "Short catchy clip title", "hook": "First sentence that grabs attention", "reason": "Why this will perform well"}]"""


METADATA_PROMPT = """You are a social media content strategist specializing in Instagram and TikTok growth.
Based on the video transcript, generate optimized metadata for maximum reach and engagement.

Return ONLY JSON, no markdown:
{
  "title": "Catchy video title under 60 chars, curiosity-driven",
  "instagram_caption": "Engaging caption with hook, value, CTA and relevant hashtags. Max 2200 chars. Use emojis naturally.",
  "instagram_hashtags": ["hashtag1", "hashtag2"],
  "tiktok_caption": "Short punchy caption under 150 chars with 3-5 hashtags. Hook in first line.",
  "tiktok_hashtags": ["hashtag1", "hashtag2"],
  "short_summary": "2 sentences describing the video content"
}

Hashtag strategy:
- Instagram: mix of niche (10k-100k), medium (100k-1M) and broad tags
- TikTok: trending + niche specific tags
- Always include topic-relevant tags, avoid generic spam tags"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run(cmd: str, desc: str = "") -> str:
    """Ğ’Ğ¸ĞºĞ¾Ğ½Ğ°Ñ‚Ğ¸ shell-ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ñ– Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ¸ stdout."""
    log.info(f"âš™ï¸  {desc or cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        log.error(f"STDERR: {result.stderr}")
        raise RuntimeError(f"Command failed: {cmd}\n{result.stderr}")
    return result.stdout.strip()


def ensure_dir(path: Path) -> Path:
    path.mkdir(parents=True, exist_ok=True)
    return path


def format_srt_time(seconds: float) -> str:
    td = timedelta(seconds=seconds)
    total_ms = int(td.total_seconds() * 1000)
    h = total_ms // 3_600_000
    m = (total_ms % 3_600_000) // 60_000
    s = (total_ms % 60_000) // 1000
    ms = total_ms % 1000
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"


def format_ass_time(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = seconds % 60
    return f"{h}:{m:02d}:{s:05.2f}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 1 â€” Extract Audio
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_audio(video_path: Path, output_dir: Path) -> Path:
    audio_path = output_dir / "audio_original.wav"
    run(
        f'ffmpeg -y -i "{video_path}" -vn -acodec pcm_s16le -ar 44100 -ac 1 "{audio_path}"',
        "Ğ’Ğ¸Ñ‚ÑĞ³ Ğ°ÑƒĞ´Ñ–Ğ¾ Ğ· Ğ²Ñ–Ğ´ĞµĞ¾",
    )
    log.info(f"âœ… ĞÑƒĞ´Ñ–Ğ¾ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {audio_path}")
    return audio_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 2 â€” Enhance Audio via Auphonic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enhance_audio_auphonic(audio_path: Path, output_dir: Path) -> Path:
    if not cfg.AUPHONIC_USER or not cfg.AUPHONIC_PASS:
        log.warning("âš ï¸  Auphonic credentials Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ñ–, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ Ğ·Ğ²ÑƒĞºÑƒ")
        return audio_path

    log.info("ğŸ”Š Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ°ÑƒĞ´Ñ–Ğ¾ Ğ² Auphonic...")
    # auth = (cfg.AUPHONIC_USER, cfg.AUPHONIC_PASS)
    headers = {"Authorization": f"Bearer {cfg.AUPHONIC_API_KEY}"}

    # Ğ—Ğ±Ğ¸Ñ€Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞµĞ½Ğ°
    data = {
        "action": "start",
        "output_basename": "enhanced",
        "algorithms": json.dumps({
            "normloudness": True,
            "denoise": True,
            "denoiseamount": 0.8,
            "hiss_reduction": True,
        }),
    }
    if cfg.AUPHONIC_PRESET:
        data["preset"] = cfg.AUPHONIC_PRESET

    with open(audio_path, "rb") as f:
        resp = requests.post(
            "https://auphonic.com/api/simple/productions.json",
            headers=headers,
            data=data,
            files={"input_file": f},
        )
    resp.raise_for_status()
    production = resp.json()["data"]
    uuid = production["uuid"]
    log.info(f"ğŸ“¤ Production UUID: {uuid}")

    # Polling Ğ´Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ
    log.info("â³ Ğ§ĞµĞºĞ°Ñ”Ğ¼Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ Auphonic (Ğ¼Ğ¾Ğ¶Ğµ Ğ·Ğ°Ğ¹Ğ½ÑÑ‚Ğ¸ Ñ…Ğ²Ğ¸Ğ»Ğ¸Ğ½Ñƒ)...")
    for _ in range(120):  # max 10 Ñ…Ğ²Ğ¸Ğ»Ğ¸Ğ½
        time.sleep(5)
        status_resp = requests.get(
            f"https://auphonic.com/api/production/{uuid}.json", headers=headers
        )
        status_resp.raise_for_status()
        prod_data = status_resp.json()["data"]
        status_code = prod_data.get("status_string", "")
        log.info(f"   Status: {status_code}")
        if prod_data["status"] == 3:  # Done
            break
        if prod_data["status"] in (9, 10):  # Error / Aborted
            raise RuntimeError(f"Auphonic failed with status: {status_code}")

    # Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ğ¼Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    output_files = prod_data.get("output_files", [])
    if not output_files:
        raise RuntimeError("Auphonic Ğ½Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ğ½ÑƒĞ² Ñ„Ğ°Ğ¹Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñƒ")

    download_url = output_files[0]["download_url"]
    enhanced_path = output_dir / "audio_enhanced.wav"

    log.info(f"ğŸ“¥ Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ñ–Ğ¾...")
    audio_resp = requests.get(download_url, headers=headers)
    audio_resp.raise_for_status()
    with open(enhanced_path, "wb") as f:
        f.write(audio_resp.content)

    log.info(f"âœ… ĞŸĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğµ Ğ°ÑƒĞ´Ñ–Ğ¾ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {enhanced_path}")
    return enhanced_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 3 â€” Merge Enhanced Audio into Video
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_audio_video(video_path: Path, audio_path: Path, output_dir: Path) -> Path:
    output_path = output_dir / "video_enhanced.mp4"
    run(
        f'ffmpeg -y -i "{video_path}" -i "{audio_path}" '
        f'-c:v copy -map 0:v:0 -map 1:a:0 -shortest "{output_path}"',
        "Ğ—Ğ»Ğ¸Ñ‚Ñ‚Ñ Ğ²Ñ–Ğ´ĞµĞ¾ Ğ· Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¼ Ğ°ÑƒĞ´Ñ–Ğ¾",
    )
    log.info(f"âœ… Ğ’Ñ–Ğ´ĞµĞ¾ Ğ· Ğ½Ğ¾Ğ²Ğ¸Ğ¼ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼: {output_path}")
    return output_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 4 â€” Transcription
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def transcribe_whisperx(audio_path: Path) -> list[dict]:
    """Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ñ–Ñ Ñ‡ĞµÑ€ĞµĞ· WhisperX (Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾). ĞŸĞ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ” ÑĞ¿Ğ¸ÑĞ¾Ğº segments."""
    try:
        import whisperx
        import torch
    except ImportError:
        raise ImportError(
            "WhisperX Ğ½Ğµ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾. Ğ’Ğ¸ĞºĞ¾Ğ½Ğ°Ğ¹: pip install whisperx torch"
        )

    device = cfg.WHISPER_DEVICE
    log.info(f"ğŸ™ï¸  WhisperX Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ñ–Ñ (model={cfg.WHISPER_MODEL}, device={device})...")

    model = whisperx.load_model(cfg.WHISPER_MODEL, device=device, compute_type="float32")
    audio = whisperx.load_audio(str(audio_path))
    result = model.transcribe(audio, language=cfg.WHISPER_LANGUAGE, batch_size=16)

    # Word-level alignment
    log.info("ğŸ”¡ Word-level alignment...")
    model_a, metadata = whisperx.load_align_model(
        language_code=cfg.WHISPER_LANGUAGE, device=device
    )
    result = whisperx.align(
        result["segments"], model_a, metadata, audio, device,
        return_char_alignments=False,
    )
    return result["segments"]

def transcribe(audio_path: Path) -> list[dict]:
    return transcribe_whisperx(audio_path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 5 â€” Fix Transcript via Claude
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fix_transcript(segments: list[dict]) -> list[dict]:
    """Ğ’Ğ¸Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ”Ğ¼Ğ¾ Ñ‚ĞµĞºÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Claude, Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ Ñ‚Ğ°Ğ¹Ğ¼-ĞºĞ¾Ğ´Ğ¸."""
    if not os.getenv("ANTHROPIC_API_KEY"):
        log.warning("âš ï¸  ANTHROPIC_API_KEY not set, skipping transcript fix")
        return segments

    log.info("âœï¸  Ğ’Ğ¸Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ‡ĞµÑ€ĞµĞ· Claude...")
    client = anthropic.Anthropic()

    # Ğ—Ğ±Ğ¸Ñ€Ğ°Ñ”Ğ¼Ğ¾ Ğ²ĞµÑÑŒ Ñ‚ĞµĞºÑÑ‚
    full_text = "\n".join(
        f"[{i}] {seg['text']}" for i, seg in enumerate(segments)
    )

    message = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=4096,
        messages=[
            {"role": "user", "content": TRANSCRIPT_FIX_PROMPT},
            {"role": "assistant", "content": "Understood. Please provide the transcript lines."},
            {"role": "user", "content": full_text},
        ],
    )

    corrected_lines = {}
    for line in message.content[0].text.strip().split("\n"):
        line = line.strip()
        if line.startswith("["):
            try:
                idx_end = line.index("]")
                idx = int(line[1:idx_end])
                text = line[idx_end + 1:].strip()
                corrected_lines[idx] = text
            except (ValueError, IndexError):
                continue

    # Ğ—Ğ°ÑÑ‚Ğ¾ÑĞ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ Ğ²Ğ¸Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ
    for i, seg in enumerate(segments):
        if i in corrected_lines:
            seg["text"] = corrected_lines[i]

    log.info("âœ… Ğ¢ĞµĞºÑÑ‚ Ğ²Ğ¸Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾")
    return segments


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 6 â€” Generate SRT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_srt(segments: list[dict], output_dir: Path) -> Path:
    srt_path = output_dir / "subtitles.srt"
    lines = []
    for i, seg in enumerate(segments, 1):
        start = format_srt_time(seg["start"])
        end = format_srt_time(seg["end"])
        text = seg["text"].strip()
        lines.append(f"{i}\n{start} --> {end}\n{text}\n")

    with open(srt_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    log.info(f"âœ… Ğ¡ÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¸ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {srt_path}")
    return srt_path


def generate_ass(segments: list[dict], output_dir: Path) -> Path:
    """Ğ“ĞµĞ½ĞµÑ€ÑƒÑ”Ğ¼Ğ¾ ASS Ğ· Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ĞºĞ¾Ñ ÑÑ‚Ğ¸Ğ»Ñ–Ğ²."""
    ass_path = output_dir / "subtitles.ass"

    # ĞŸĞ¾Ğ·Ğ¸Ñ†Ñ–Ñ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ñ–Ğ²
    alignment_map = {"bottom": 2, "top": 8, "center": 5}
    alignment = alignment_map.get(cfg.SUBTITLE_POSITION, 2)

    header = f"""[Script Info]
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
PlayResX: 1920
PlayResY: 1080

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,{cfg.SUBTITLE_FONT},{cfg.SUBTITLE_FONT_SIZE},{cfg.SUBTITLE_COLOR},&H000000FF,{cfg.SUBTITLE_OUTLINE_COLOR},&H00000000,0,0,0,0,100,100,0,0,1,{cfg.SUBTITLE_OUTLINE_SIZE},0,{alignment},10,10,30,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
    events = []
    for seg in segments:
        start = format_ass_time(seg["start"])
        end = format_ass_time(seg["end"])
        text = seg["text"].strip().replace("\n", "\\N")
        events.append(f"Dialogue: 0,{start},{end},Default,,0,0,0,,{text}")

    with open(ass_path, "w", encoding="utf-8") as f:
        f.write(header + "\n".join(events))

    log.info(f"âœ… ASS ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¸ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {ass_path}")
    return ass_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 7 â€” Burn Subtitles into Video
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def burn_subtitles(video_path: Path, srt_path: Path, output_dir: Path) -> Path:
    output_path = output_dir / "video_subtitled.mp4"

    # ASS Ğ´Ğ°Ñ” ĞºÑ€Ğ°Ñ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ·Ğ° ÑÑ‚Ğ¸Ğ»ĞµĞ¼
    ass_path = srt_path.with_suffix(".ass")
    if ass_path.exists():
        subtitle_filter = f"ass='{ass_path}'"
    else:
        style = (
            f"FontName={cfg.SUBTITLE_FONT},"
            f"FontSize={cfg.SUBTITLE_FONT_SIZE},"
            f"PrimaryColour={cfg.SUBTITLE_COLOR},"
            f"OutlineColour={cfg.SUBTITLE_OUTLINE_COLOR},"
            f"Outline={cfg.SUBTITLE_OUTLINE_SIZE}"
        )
        subtitle_filter = f"subtitles='{srt_path}':force_style='{style}'"

    run(
        f'ffmpeg -y -i "{video_path}" -vf "{subtitle_filter}" '
        f'-c:a copy "{output_path}"',
        "ĞĞ°ĞºĞ»Ğ°Ğ´Ğ°Ğ½Ğ½Ñ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ñ–Ğ² Ğ½Ğ° Ğ²Ñ–Ğ´ĞµĞ¾",
    )
    log.info(f"âœ… Ğ’Ñ–Ğ´ĞµĞ¾ Ğ· ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ°Ğ¼Ğ¸: {output_path}")
    return output_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 8 â€” Format Video (padding / crop)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ASPECT_RATIOS = {
    "16:9": (1920, 1080),
    "9:16": (1080, 1920),
    "1:1": (1080, 1080),
    "4:5": (1080, 1350),
}


def format_video(video_path: Path, output_dir: Path, suffix: str = "formatted") -> Path:
    if cfg.OUTPUT_FORMAT == "original" and not cfg.ADD_PADDING:
        log.info("â© Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾ (original)")
        return video_path

    output_path = output_dir / f"video_{suffix}.mp4"
    target = ASPECT_RATIOS.get(cfg.OUTPUT_FORMAT, (1920, 1080))
    w, h = target

    if cfg.ADD_PADDING:
        # Scale + pad (Ğ´Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾Ğ»Ñ)
        vf = (
            f"scale={w}:{h}:force_original_aspect_ratio=decrease,"
            f"pad={w}:{h}:(ow-iw)/2:(oh-ih)/2:{cfg.PADDING_COLOR}"
        )
    else:
        # Scale + crop (Ğ¾Ğ±Ñ€Ñ–Ğ·Ğ°Ñ”Ğ¼Ğ¾)
        vf = f"scale={w}:{h}:force_original_aspect_ratio=increase,crop={w}:{h}"

    run(
        f'ffmpeg -y -i "{video_path}" -vf "{vf}" -c:a copy "{output_path}"',
        f"Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ²Ñ–Ğ´ĞµĞ¾ â†’ {cfg.OUTPUT_FORMAT}",
    )
    log.info(f"âœ… Ğ’Ñ–Ğ´Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğµ Ğ²Ñ–Ğ´ĞµĞ¾: {output_path}")
    return output_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 9 â€” Generate Reels via Claude
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_reels_timestamps(segments: list[dict]) -> list[dict]:
    """ĞŸÑ€Ğ¾ÑĞ¸Ğ¼Ğ¾ Claude Ğ²Ğ¸Ğ±Ñ€Ğ°Ñ‚Ğ¸ Ğ½Ğ°Ğ¹ĞºÑ€Ğ°Ñ‰Ñ– Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¸ Ğ´Ğ»Ñ Reels."""
    if not os.getenv("ANTHROPIC_API_KEY"):
        log.warning("âš ï¸  ANTHROPIC_API_KEY not set, using fallback reel splitting")
        return _fallback_reels(segments)

    log.info("ğŸ¬ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ Ñ‚Ğ°Ğ¹Ğ¼-ĞºĞ¾Ğ´Ñ–Ğ² Ğ´Ğ»Ñ Reels Ñ‡ĞµÑ€ĞµĞ· Claude...")
    client = anthropic.Anthropic()

    transcript_with_times = "\n".join(
        f"[{seg['start']:.1f}s - {seg['end']:.1f}s]: {seg['text']}"
        for seg in segments
    )

    user_prompt = (
        f"Here is the video transcript with timestamps:\n{transcript_with_times}\n\n"
        f"Find the {cfg.REELS_COUNT} best moments, each between {cfg.REELS_MIN_DURATION} and "
        f"{cfg.REELS_MAX_DURATION} seconds long."
    )
    message = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=1024,
        messages=[
            {"role": "user", "content": REELS_SELECTION_PROMPT},
            {"role": "assistant", "content": "Understood. Please provide the transcript with timestamps."},
            {"role": "user", "content": user_prompt},
        ],
    )

    raw = message.content[0].text.strip()
    # Ğ’Ğ¸Ğ´Ğ°Ğ»ÑÑ”Ğ¼Ğ¾ Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ñ– markdown-Ñ‚ĞµĞ³Ğ¸
    raw = raw.replace("```json", "").replace("```", "").strip()
    try:
        reels = json.loads(raw)
        log.info(f"âœ… Claude Ğ·Ğ°Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ğ½ÑƒĞ²Ğ°Ğ² {len(reels)} Reels")
        return reels
    except json.JSONDecodeError:
        log.warning("âš ï¸  ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ñ€Ğ¾Ğ·Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚Ğ¸ JSON Ğ²Ñ–Ğ´ Claude, Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ fallback")
        return _fallback_reels(segments)


def _fallback_reels(segments: list[dict]) -> list[dict]:
    """Ğ Ñ–Ğ²Ğ½Ğ¾Ğ¼Ñ–Ñ€Ğ½Ğ° Ğ½Ğ°Ñ€Ñ–Ğ·ĞºĞ° ÑĞºÑ‰Ğ¾ Claude Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ğ¹."""
    if not segments:
        return []
    total_duration = segments[-1]["end"]
    step = total_duration / cfg.REELS_COUNT
    reels = []
    for i in range(cfg.REELS_COUNT):
        start = i * step
        end = min(start + cfg.REELS_MAX_DURATION, total_duration)
        reels.append({"start": start, "end": end, "title": f"Reel {i+1}", "reason": "auto"})
    return reels


def cut_reels(video_path: Path, reels: list[dict], output_dir: Path) -> list[Path]:
    reels_dir = ensure_dir(output_dir / "reels")
    reel_paths = []
    for i, reel in enumerate(reels, 1):
        start = reel["start"]
        end = reel["end"]
        title_slug = reel.get("title", f"reel_{i}").replace(" ", "_")[:30]
        out = reels_dir / f"reel_{i:02d}_{title_slug}.mp4"

        run(
            f'ffmpeg -y -i "{video_path}" -ss {start:.2f} -to {end:.2f} -c copy "{out}"',
            f"ĞĞ°Ñ€Ñ–Ğ·ĞºĞ° Reel {i}: {reel.get('title', '')}",
        )
        reel_paths.append(out)

    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒÑ”Ğ¼Ğ¾ Reels Ñƒ 9:16 ÑĞºÑ‰Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾
    if cfg.OUTPUT_FORMAT != "original":
        formatted_reels = []
        for reel_path in reel_paths:
            formatted = format_video(reel_path, reels_dir, suffix=reel_path.stem + "_9x16")
            formatted_reels.append(formatted)
        return formatted_reels

    log.info(f"âœ… ĞĞ°Ñ€Ñ–Ğ·Ğ°Ğ½Ğ¾ {len(reel_paths)} Reels")
    return reel_paths


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 10 â€” Generate Metadata via Claude
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_metadata(segments: list[dict], output_dir: Path) -> dict:
    if not os.getenv("ANTHROPIC_API_KEY"):
        log.warning("âš ï¸  ANTHROPIC_API_KEY not set, skipping metadata generation")
        return {}

    log.info("ğŸ“‹ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Claude...")
    client = anthropic.Anthropic()

    # Ğ‘ĞµÑ€ĞµĞ¼Ğ¾ Ğ¿ĞµÑ€ÑˆÑ– 3000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ–Ğ² Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñƒ
    transcript_preview = " ".join(seg["text"] for seg in segments)[:3000]

    message = client.messages.create(
        model="claude-sonnet-4-6",
        max_tokens=1500,
        messages=[
            {"role": "user", "content": METADATA_PROMPT},
            {"role": "assistant", "content": "Understood. Please provide the video transcript."},
            {"role": "user", "content": f"Video transcript:\n{transcript_preview}"},
        ],
    )

    raw = message.content[0].text.strip()
    raw = raw.replace("```json", "").replace("```", "").strip()
    try:
        metadata = json.loads(raw)
    except json.JSONDecodeError:
        log.warning("âš ï¸  ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ñ€Ğ¾Ğ·Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ñ–")
        metadata = {"raw": raw}

    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    log.info(f"âœ… ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ñ– Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {metadata_path}")
    return metadata


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main Pipeline
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_pipeline(input_video: str, output_dir: str, steps: list[str] = None):
    video_path = Path(input_video).resolve()
    if not video_path.exists():
        raise FileNotFoundError(f"Ğ’Ñ–Ğ´ĞµĞ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {video_path}")

    out = ensure_dir(Path(output_dir))
    log.info(f"ğŸš€ ĞŸĞ¾Ñ‡Ğ¸Ğ½Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ: {video_path.name}")
    log.info(f"ğŸ“ Ğ’Ğ¸Ñ…Ñ–Ğ´Ğ½Ğ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ: {out}")

    all_steps = ["audio", "enhance", "merge", "transcribe", "fix", "subtitles", "format", "reels", "metadata"]
    active = set(steps) if steps else set(all_steps)

    # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ ÑÑ‚Ğ°Ğ½ Ğ¼Ñ–Ğ¶ ĞºÑ€Ğ¾ĞºĞ°Ğ¼Ğ¸
    state = {"video": video_path}

    # 1. Ğ’Ğ¸Ñ‚ÑĞ³ Ğ°ÑƒĞ´Ñ–Ğ¾
    if "audio" in active:
        state["audio"] = extract_audio(state["video"], out)

    # 2. ĞŸĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ½Ñ Ğ·Ğ²ÑƒĞºÑƒ
    if "enhance" in active and "audio" in state:
        state["enhanced_audio"] = enhance_audio_auphonic(state["audio"], out)
    else:
        state["enhanced_audio"] = state.get("audio", video_path)

    # 3. Ğ—Ğ»Ğ¸Ñ‚Ñ‚Ñ Ğ²Ñ–Ğ´ĞµĞ¾ + Ğ°ÑƒĞ´Ñ–Ğ¾
    if "merge" in active and state["enhanced_audio"] != state.get("audio"):
        state["video"] = merge_audio_video(state["video"], state["enhanced_audio"], out)
    else:
        # ĞšĞ¾Ğ¿Ñ–ÑÑ”Ğ¼Ğ¾ Ğ²Ñ…Ñ–Ğ´Ğ½Ğµ Ğ²Ñ–Ğ´ĞµĞ¾ Ñƒ out Ğ´Ğ»Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ñ–
        dest = out / video_path.name
        if not dest.exists():
            shutil.copy2(video_path, dest)
        state["video"] = dest

    # 4. Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ñ–Ñ
    if "transcribe" in active:
        state["segments"] = transcribe(state.get("enhanced_audio", state["audio"]))
        # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ ÑĞ¸Ñ€Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚
        raw_path = out / "transcript_raw.json"
        with open(raw_path, "w", encoding="utf-8") as f:
            json.dump(state["segments"], f, ensure_ascii=False, indent=2)
        log.info(f"ğŸ’¾ Ğ¡Ğ¸Ñ€Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚: {raw_path}")

    # 5. Ğ’Ğ¸Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ Ñ‚ĞµĞºÑÑ‚Ñƒ
    if "fix" in active and "segments" in state:
        state["segments"] = fix_transcript(state["segments"])
        fixed_path = out / "transcript_fixed.json"
        with open(fixed_path, "w", encoding="utf-8") as f:
            json.dump(state["segments"], f, ensure_ascii=False, indent=2)

    # 6. Ğ¡ÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¸
    if "subtitles" in active and "segments" in state:
        srt_path = generate_srt(state["segments"], out)
        ass_path = generate_ass(state["segments"], out)
        state["video"] = burn_subtitles(state["video"], srt_path, out)

    # 7. Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ²Ñ–Ğ´ĞµĞ¾
    if "format" in active:
        state["video"] = format_video(state["video"], out)

    # 8. Reels
    if "reels" in active and "segments" in state:
        reels_timestamps = get_reels_timestamps(state["segments"])
        reels_info_path = out / "reels_timestamps.json"
        with open(reels_info_path, "w", encoding="utf-8") as f:
            json.dump(reels_timestamps, f, ensure_ascii=False, indent=2)
        state["reels"] = cut_reels(state["video"], reels_timestamps, out)

    # 9. ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ñ–
    if "metadata" in active and "segments" in state:
        state["metadata"] = generate_metadata(state["segments"], out)
        if state["metadata"]:
            print("\n" + "=" * 50)
            print("ğŸ“‹ GENERATED METADATA:")
            print("=" * 50)
            list_fields = {"instagram_hashtags", "tiktok_hashtags"}
            for k, v in state["metadata"].items():
                if k in list_fields:
                    print(f"\nğŸ”¹ {k.upper()}: {', '.join(v)}")
                else:
                    print(f"\nğŸ”¹ {k.upper()}:\n{v}")

    # Ğ¤Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ğ·Ğ²Ñ–Ñ‚
    final_video = state["video"]
    log.info("\n" + "=" * 50)
    log.info("ğŸ‰ ĞŸĞĞ™ĞŸĞ›ĞĞ™Ğ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!")
    log.info(f"ğŸ“¹ Ğ¤Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğµ Ğ²Ñ–Ğ´ĞµĞ¾: {final_video}")
    log.info(f"ğŸ“ Ğ’ÑÑ– Ñ„Ğ°Ğ¹Ğ»Ğ¸: {out}")
    if "reels" in state:
        log.info(f"âœ‚ï¸  Reels: {len(state['reels'])} Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ² Ñƒ {out / 'reels'}")

    return state


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(
        description="ĞŸĞ¾Ğ²Ğ½Ğ¸Ğ¹ Ğ²Ñ–Ğ´ĞµĞ¾-Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½: Ğ°ÑƒĞ´Ñ–Ğ¾ â†’ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ñ–Ñ â†’ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¸ â†’ Reels"
    )
    parser.add_argument("input", help="Ğ¨Ğ»ÑÑ… Ğ´Ğ¾ Ğ²Ñ…Ñ–Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ–Ğ´ĞµĞ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ")
    parser.add_argument(
        "-o", "--output", default="./output", help="Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ² (default: ./output)"
    )
    parser.add_argument(
        "--steps",
        nargs="+",
        choices=["audio", "enhance", "merge", "transcribe", "fix", "subtitles", "format", "reels", "metadata"],
        help="Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ²ĞºĞ°Ğ·Ğ°Ğ½Ñ– ĞºÑ€Ğ¾ĞºĞ¸ (default: Ğ²ÑÑ–)",
    )
    args = parser.parse_args()
    run_pipeline(args.input, args.output, args.steps)


if __name__ == "__main__":
    main()
